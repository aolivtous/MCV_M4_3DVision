{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>**Lab 4: 3D Reconstruction from two images with known internal parameters</u>**\n",
    "\n",
    "In this lab you will learn how to triangulate the matching correspondences between two views of the same scene, by means of applying the Direct Linear Method (DLT). Then you will compute the camera matrices of these images given the Fundamental Matrix and the Intrinsic Matrix, and will evaluate your triangulation method on these camera matrices by estimating the reprojection error in the triangulation. \n",
    "\n",
    "The last two tasks will involve the computation of depth maps by local methods (SSD, NCC), with their evaluation, and the implementation of bilateral weights on this mapping. \n",
    "\n",
    "The following file combines some text cells (Markdown cells) and code cells. Some parts of the code need to be completed. All tasks you need to complete are marked in <span style='color:Green'> green.  </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import cv2\n",
    "import math\n",
    "import sys\n",
    "import random\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = (19, 10)\n",
    "from operator import itemgetter\n",
    "from plotly import graph_objects as go\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "from time import time \n",
    "from functools import partial\n",
    "\n",
    "import utils\n",
    "from utils import homogeneous2euclidean, euclidean2homogeneous, projective2img, img2projective\n",
    "import aux_code as aux\n",
    "\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Triangulation with the DLT method**\n",
    "\n",
    "The first task is to implement the DLT method that will compute the 3D points corresponding to the matches of two images whose camera matrices are known. [See derivation of DLT here](https://www.cs.cmu.edu/~16385/s17/Slides/11.4_Triangulation.pdf)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/8356912/151347636-c3332917-a7da-402e-9eb6-f39beb91591a.png)\n",
    "\n",
    "\n",
    "<span style='color:Green'> - Create the function triangulate(x1, x2, P1, P2, imsize) that performs a triangulation with the homogeneous algebraic method (DLT)  </span>\n",
    "\n",
    "The entries are (x1, x2, P1, P2, imsize), where:\n",
    "- x1: array of shape (3, `n_points`), containing matching points in projective coordinates from Camera 1 reference frame \n",
    "- x2: array of shape (3, `n_points`), containing matching points in projective coordinates from Camera 2 reference frame.\n",
    "- P1: Camera 1 matrix of shape (3,4)\n",
    "- P2: Camera 2 matrix of shape (3,4)\n",
    "- imsize: Iterable with shape (2,) with the image size\n",
    "\n",
    "The function should return:\n",
    "- X: array of shape (4, `n_points`), containing the Homogenous Coordinates of the points in 3D space.  \n",
    "\n",
    "Test the triangulate function: use this code to validate that the function triangulate works properly\n",
    "\n",
    "HINT: check numpy.linalg.svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write here the method for DLT triangulation\n",
    "def triangulate(x1, x2, P1, P2, imsize) -> np.ndarray:\n",
    "    assert P1.shape == (3,4) == P2.shape\n",
    "    assert x1.shape == x2.shape and x1.shape[0] == 3\n",
    "    '''\n",
    "    Your implementation here\n",
    "    '''\n",
    "    raise NotImplementedError(\"You must implement this, don't forget\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the triangulation function with two Cameras and some random points. The average triangulation error should be lower than 10<sup>-8</sup>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "P1 = np.zeros((3,4))\n",
    "P1[0,0]=P1[1,1]=P1[2,2]=1                            # Cam1 Canonical Camera Matrix\n",
    "\n",
    "angle = 15\n",
    "theta = np.radians(angle)\n",
    "c = np.cos(theta)\n",
    "s = np.sin(theta)\n",
    "R = np.array([[c, -s, 0], [s, c, 0], [0, 0, 1]])     # Cam2 Rotation_z(theta[deg])\n",
    "t = np.array([[1.1, .1, .2]])                         # Cam2 Translation(x[m], y[m], z[m]) \n",
    "P2 = np.concatenate((R, t.T), axis=1)\n",
    "\n",
    "n_points = 20                                        # Number of data samples (points) (x,y,z)\n",
    "X_eucl = np.random.uniform(0, 1, (3,n_points))       # Random points in 3D space on world reference frame\n",
    "X_eucl[2,:] += 3                                     # Make points fit into the camera frame \n",
    "X = euclidean2homogeneous(X_eucl)                    # \n",
    "\n",
    "# Obtain the camera coordinates (rays) from each point.\n",
    "x_proj1 = P1 @ X                                     # Project to Cam1 image coordinates (x,y,z)->[x_c1:y_c1:z_c1]\n",
    "x_proj2 = P2 @ X                                     # Project to Cam2 image coordinates (x,y,z)->[x_c2:y_c2:z_c2]\n",
    "\n",
    "# Estimate the 3D points (you need to create this function)\n",
    "X_trian = triangulate(x_proj1, x_proj2, P1, P2, ((2,2)));\n",
    "assert X_trian.shape == (4, n_points)\n",
    "\n",
    "# Evaluation: compute the reprojection error\n",
    "X_eucl_pred = homogeneous2euclidean(X_trian)\n",
    "X_eucl = homogeneous2euclidean(X)\n",
    "errors = np.linalg.norm(X_eucl - X_eucl_pred, axis=0)\n",
    "avg_error = np.mean(errors)\n",
    "print(f\"Average triangulation error: {avg_error:.2e}[m]\")\n",
    "assert avg_error < 1e-8, \"Something might be wrong here\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will plot the results to check that the triangulated values are close to the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "utils.draw_points(X_eucl.T, ax=ax, c=\"k\", alpha=0.5, s=100, depthshade=True, label=\"Groud Truth\")\n",
    "utils.draw_points(X_eucl_pred.T, ax=ax, c=\"r\", alpha=1, s=5, depthshade=True, label=\"Triangulated\")\n",
    "fig.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(layout=dict(height=400, width=550))\n",
    "fig.add_trace(go.Scatter3d(x=X_eucl[0,:], y=X_eucl[1,:], z=X_eucl[2,:], mode='markers', marker=dict(size=6, color=\"black\", opacity=0.5), name='Ground Truth'))\n",
    "fig.add_trace(go.Scatter3d(x=X_eucl_pred[0,:], y=X_eucl_pred[1,:], z=X_eucl_pred[2,:], mode='markers', marker=dict(size=2, color=\"red\", opacity=1), name='Triangulated'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. **[Reconstruction](https://www.cs.cmu.edu/~16385/s17/Slides/12.5_Reconstruction.pdf) from two views**\n",
    "\n",
    "The goal of this section is to estimate the 3D reconstruction from two views in a practical situation where the image correspondences contain outliers. \n",
    "\n",
    "To acomplish this you have to:\n",
    "\n",
    "1. Find a set of point correspondences between the two images. This correspondences will contain noise/outliers, therefore we cannot use [Epipolar Geometry](https://www.cs.cmu.edu/~16385/s17/Slides/12.1_Epipolar_Geometry.pdf) to calculate the [Essential Matrix](https://www.cs.cmu.edu/~16385/s17/Slides/12.2_Essential_Matrix.pdf), but we can find the Fundamental Matrix. \n",
    "2. Estimate the [Fundamental Matrix](https://www.cs.cmu.edu/~16385/s17/Slides/12.3_Fundamental_Matrix.pdf) relating the two camera images. Since correspondances are noisy we will use the [8-point Algorithm](https://www.cs.cmu.edu/~16385/s17/Slides/12.4_8Point_Algorithm.pdf) in combination with the RANSAC algorithm, to find a good estimate of $F$.\n",
    "3. Calculate the Essential Matrix from the Fundamental Matrix\n",
    "4. Find the two Camera Matrices.\n",
    "## 2.1 Estimate the image matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read images\n",
    "img1 = cv2.imread('Data/0001_s.png',cv2.IMREAD_GRAYSCALE)\n",
    "img2 = cv2.imread('Data/0002_s.png',cv2.IMREAD_GRAYSCALE)\n",
    "h, w = img1.shape\n",
    "\n",
    "# Initiate ORB detector\n",
    "orb = cv2.ORB_create(3000)\n",
    "# find the keypoints and descriptors with ORB\n",
    "kp1, des1 = orb.detectAndCompute(img1,None)\n",
    "kp2, des2 = orb.detectAndCompute(img2,None)\n",
    "\n",
    "# Keypoint matching\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "matches = bf.match(des1,des2)\n",
    "\n",
    "# Show \"good\" matches\n",
    "img_12 = cv2.drawMatches(img1,kp1,img2,kp2,matches,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "fig = plt.figure()\n",
    "plt.imshow(img_12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Estimate the Fundamental Matrix\n",
    "To find the Fundamental Matrix $F$ we will use the 8-point algorithm. However, using only 8 correspondances might lead to bad estimates of $F$. To find a robust estimate we will use the RANSAC algorithm. In simple words the algorithm samples randomly 8 correspondances and estimates $F_{estimated}$ for these samples, then it evaluates how well does $F_{estimated}$ fit the rest of the data. We repreat this process until a limit of iterations is reached and select the best estimated $F$.  \n",
    "\n",
    "(Code is provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust estimation of the fundamental matrix #\n",
    "points1 = []\n",
    "points2 = []\n",
    "for m in matches:\n",
    "    points1.append([kp1[m.queryIdx].pt[0], kp1[m.queryIdx].pt[1], 1])\n",
    "    points2.append([kp2[m.trainIdx].pt[0], kp2[m.trainIdx].pt[1], 1])\n",
    "    \n",
    "points1 = np.asarray(points1)\n",
    "points1 = points1.T\n",
    "points2 = np.asarray(points2)\n",
    "points2 = points2.T\n",
    "\n",
    "F, indices_inlier_matches = aux.Ransac_fundamental_matrix(points1, points2, 1, 5000)\n",
    "inlier_matches = itemgetter(*indices_inlier_matches)(matches)\n",
    "\n",
    "img_12 = cv2.drawMatches(img1,kp1,img2,kp2,inlier_matches,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "fig = plt.figure()\n",
    "plt.imshow(img_12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Estimate the Essential Matrix\n",
    "<span style='color:Green'> - Compute the [Essential matrix](https://www.cs.cmu.edu/~16385/s17/Slides/12.2_Essential_Matrix.pdf) from the Fundamental matrix </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Camera calibration matrix\n",
    "K = np.array([[2362.12, 0, 1520.69], [0, 2366.12, 1006.81], [0, 0, 1]])\n",
    "scale = 0.3;\n",
    "H = np.array([[scale, 0, 0], [ 0, scale, 0], [0, 0, 1]])\n",
    "K = H @ K;\n",
    "\n",
    "def essential_from_fundamental(K, F):\n",
    "    raise NotImplementedError()\n",
    "\n",
    "E = essential_from_fundamental(K, F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Estimate the Camera Matrices from the Essential Matrix\n",
    "<span style='color:Green'> - Estimate the camera projection matrices. Assuming the first Camera has a cannonical matrix you will obtain four possible Camera Matrices for the second camera, two rotations and two translations.</span>\n",
    "\n",
    " - Please explain how do you obtain this estimates, not what algorithm or methodology you used to obtain them. i.e., What system of equations are you solving?\n",
    " - Why do you obtain 4 solutions? Why not only one?\n",
    "\n",
    "HINT: Once you estimate the rotations $R$ and translation $T$ of the second camera matrix, you may get as results improper rotations: $|R| = -1$ i.e., $R \\in O(3), R \\notin SO(3)$. <span style='color:Green'> (Why do you think this happens?) </span> In that case you should make the rotation proper by avoiding the unwanted reflexion (multiply by -1 so $|R| = 1$): e.g.,\n",
    "  \n",
    "      if det(R) < 0:  \n",
    "          R = -R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # camera projection matrix for the first camera\n",
    "# P1 = ...\n",
    "\n",
    "# # four possible matrices for the second camera\n",
    "# Pc2 = [None, None, None, None];\n",
    "# Pc2[0] = ...\n",
    "# Pc2[1] = ...\n",
    "# Pc2[2] = ...\n",
    "# Pc2[3] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualize the 4 possible Camera positions and orientations, in relation to Camera 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the first camera and the four possible solutions for the second figure:\n",
    "fig = plt.figure()\n",
    "axs = [fig.add_subplot(1,5,i+1, projection='3d') for i in range(5)]\n",
    "titles = [\"Cam 1\"] + [r\"Cam $2_{%s}$\" % int(i+1) for i in range(len(Pc2)) ]\n",
    "\n",
    "low_lim, up_lim = -1.5, 1.5\n",
    "for ax, title, P in zip(axs, titles, [P1] + Pc2):\n",
    "    utils.plot_camera(P,w,h, 1, ax=ax)\n",
    "    utils.plot_camera(P1,w,h, 1, ax=ax, alpha=0.3, color=\"k\") # Show Cam1 for reference\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlim(low_lim, up_lim)\n",
    "    ax.set_ylim(low_lim, up_lim)\n",
    "    ax.set_zlim(low_lim, up_lim)\n",
    "    \n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = fig.add_subplot(1,1,1, projection='3d')\n",
    "for P, color in zip(Pc2, ['r','k','b','g']):\n",
    "    utils.plot_camera(P,w,h, 1, ax=ax, color=color)\n",
    "utils.plot_camera(P1,w,h, 1, ax=ax, alpha=0.3, color=\"k\") # Show Cam1 for reference\n",
    "ax.set_xlim(low_lim, up_lim)\n",
    "ax.set_ylim(low_lim, up_lim)\n",
    "ax.set_zlim(low_lim, up_lim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:Green'> - Triangulate the correspondances and select the \"optimal\" camera, how do you select it ?</span>\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv2.imread('Data/0000_s.png',cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "\n",
    "# Obtain the inlier points projective coordinates for Camera 1 and 2 \n",
    "x1 = ...\n",
    "x2 = ...\n",
    "\n",
    "# Prepare figure \n",
    "fig = plt.figure(figsize=(15,15))\n",
    "axs = [fig.add_subplot(2,2,i+1, projection='3d') for i in range(4)]\n",
    "\n",
    "# Variable for optimal camera\n",
    "P2_selected = None\n",
    "\n",
    "low_lim, up_lim = -10, 10\n",
    "for i, ax, P2 in zip(range(len(Pc2)), axs, Pc2):\n",
    "    # Estimate by triangulization the 3D coordinates of the matches\n",
    "    X_pred = triangulate(x1, x2, P1, P2, [w, h])\n",
    "    X_eucl = homogeneous2euclidean(X_pred)\n",
    "    \n",
    "    # Render the 3D point cloud\n",
    "    x_img = np.transpose(x1[:2]).astype(int)\n",
    "    rgb_txt = (img1[x_img[:,1], x_img[:,0]])/255\n",
    "    utils.draw_points(X_eucl.T, color=rgb_txt, ax=ax)\n",
    "    utils.plot_camera(P1,w,h,1, ax=ax)\n",
    "    utils.plot_camera(P2,w,h,1, ax=ax)\n",
    "    ax.set_title(r\"Cam $2_{%d}$\" % (i+1))\n",
    "    ax.set_xlim(low_lim, up_lim)\n",
    "    ax.set_ylim(low_lim, up_lim)\n",
    "    ax.set_zlim(low_lim, up_lim)\n",
    "    \n",
    "    # Select the best camera using some metric \n",
    "    some_metric = True:\n",
    "    if some_metric:\n",
    "        P2_selected = P2\n",
    "\n",
    "# From now own P2 will be the camera you selected\n",
    "P2 = P2_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 3D Visualization\n",
    " We will visualize the 3D points for the selected camera with an interactive plot so that you can see the surfaces reconstructed in 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate by triangulization the 3D coordinates of the matches\n",
    "X_pred = triangulate(x1, x2, P1, P2, [w, h])\n",
    "X_eucl = X_pred/X_pred[-1,:]\n",
    "points3d = X_eucl[:3].T\n",
    "fig = go.Figure(layout=dict(height=400, width=550))\n",
    "points3d = X_eucl[:3].T\n",
    "fig.add_trace(go.Scatter3d(x=points3d[:,0], y=points3d[:,1], z=points3d[:,2],mode='markers',name='3d points',marker=dict(color=rgb_txt, size=2)))\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate by triangulization the 3D coordinates of the matches\n",
    "X_pred = triangulate(x1, x2, P1, P2, [w, h])\n",
    "X_eucl = X_pred/X_pred[-1,:]\n",
    "points3d = X_eucl[:3].T\n",
    "fig = go.Figure(layout=dict(height=400, width=550))\n",
    "points3d = X_eucl[:3].T\n",
    "fig.add_trace(go.Scatter3d(x=points3d[:,0], y=points3d[:,1], z=points3d[:,2],mode='markers',name='3d points',marker=dict(color=rgb_txt, size=2)))\n",
    "fig.show()\n",
    "# Estimate by triangulization the 3D coordinates of the matches\n",
    "X_pred = triangulate(x1, x2, P1, P2, [w, h])\n",
    "X_eucl = X_pred/X_pred[-1,:]\n",
    "points3d = X_eucl[:3].T\n",
    "fig = go.Figure(layout=dict(height=400, width=550))\n",
    "points3d = X_eucl[:3].T\n",
    "fig.add_trace(go.Scatter3d(x=points3d[:,0], y=points3d[:,1], z=points3d[:,2],mode='markers',name='3d points',marker=dict(color=rgb_txt, size=2)))\n",
    "fig.show()\n",
    "# Estimate by triangulization the 3D coordinates of the matches\n",
    "X_pred = triangulate(x1, x2, P1, P2, [w, h])\n",
    "X_eucl = X_pred/X_pred[-1,:]\n",
    "points3d = X_eucl[:3].T\n",
    "fig = go.Figure(layout=dict(height=400, width=550))\n",
    "points3d = X_eucl[:3].T\n",
    "fig.add_trace(go.Scatter3d(x=points3d[:,0], y=points3d[:,1], z=points3d[:,2],mode='markers',name='3d points',marker=dict(color=rgb_txt, size=2)))\n",
    "fig.show()\n",
    "#### And these are the corresponding points in 2D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_with_selected_points = copy.deepcopy(img1)\n",
    "img_with_selected_points[x_img[:,1], x_img[:,0]] = [255,0,0]\n",
    "plt.figure(figsize = (80,40))\n",
    "plt.imshow(img_with_selected_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Reprojection Error\n",
    "\n",
    "Once you selected a Camera, analyze the error of reprojecting the 3D space back to image coordinates. \n",
    "\n",
    "<span style='color:Green'>- Compute the reprojection errors  </span>\n",
    "<span style='color:Green'>- Plot the histogram of reprojection errors, </span>\n",
    "<span style='color:Green'>- Plot the mean reprojection error  </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reprojection_errors(x1, x2, P1, P2, X):\n",
    "    \"\"\"\n",
    "    - x1: array of shape (3, `n_points`), containing matching points in projective coordinates from Camera 1 reference frame \n",
    "    - x2: array of shape (3, `n_points`), containing matching points in projective coordinates from Camera 2 reference frame.\n",
    "    - P1: Camera 1 matrix\n",
    "    - P2: Camera 2 matrix \n",
    "    - X: (4, `n_points`) homogenous coordinates of points in 3D space\n",
    "    Compute the reprojection error from `X` to each of the camera's projective coordinates\n",
    "    @return: (cam1_errors, cam2_errors)\n",
    "    \"\"\"\n",
    "    \n",
    "    '''\n",
    "    Your implementation here\n",
    "    '''\n",
    "    raise NotImplementedError(\"You must implement this, don't forget\")\n",
    "\n",
    "    return cam1_errors, cam2_errors\n",
    "\n",
    "def plot_error_stats(cam1_errors, cam2_errors):\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,4))\n",
    "    \n",
    "    ax1.hist(cam1_errors, color='c')\n",
    "    ax1.grid(axis='y', alpha=0.2)\n",
    "    ax1.set_xlabel(\"Reprojection Error Cam 1 [What units am I?]\")\n",
    "    ax2.hist(cam2_errors, color='k')\n",
    "    ax2.grid(axis='y', alpha=0.2)\n",
    "    ax2.set_xlabel(\"Reprojection Error Cam 2 [What units am I?]\")\n",
    "\n",
    "\n",
    "# Get projections with selected Camera \n",
    "X_pred = ...\n",
    "\n",
    "cam1_errors, cam2_errors = reprojection_errors(x1, x2, P1, P2, X_pred)\n",
    "\n",
    "plot_error_stats(cam1_errors, cam2_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Depth map computation using local methods**\n",
    "\n",
    "## 3.1 Vectorization (hint)\n",
    "\n",
    "Vectorization is important to get efficient implementations that will allow you to run experiments in a reasonable amount of time\n",
    "\n",
    "We will show you an example dummy computation using a naive implementation, a parallelized implementation and a vectorized implementation to see the importance of an efficient implementation and some ideas that you may reuse in the lab. This is just an example and there are different ways to vectorize, so I would suggest **reading about vectorization in python and finding your way.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_row_calculation(row, N, height, width, padding, window_size):\n",
    "    count = 0.\n",
    "    N = window_size**2*(height-2*padding)*(width-2*padding)\n",
    "    for column in range(padding, width-padding):\n",
    "        for i in range(window_size):\n",
    "            for j in range(window_size):\n",
    "                count += (left_img[row+i-padding, column+j-padding] -\n",
    "                           right_img[row+i-padding, column+j-padding]) / N\n",
    "    return count \n",
    "\n",
    "def dummy_computation(left_img, right_img, window_size):\n",
    "    assert (window_size%2==1)\n",
    "    assert (left_img.shape == right_img.shape)\n",
    "    height, width = left_img.shape[0], left_img.shape[1]\n",
    "    padding = window_size//2\n",
    "    count = 0.\n",
    "    N = window_size**2*(height-2*padding)*(width-2*padding)\n",
    "    for row in tqdm(range(padding, height-padding)):\n",
    "        count += add_row_calculation(row, N=N, height=height, width=width, padding=padding, window_size=window_size)\n",
    "    return count\n",
    "    \n",
    "def dummy_computation_parallelized(left_img, right_img, window_size, num_process=12):\n",
    "    assert (window_size%2==1)\n",
    "    assert (left_img.shape == right_img.shape)\n",
    "    height, width = left_img.shape[0], left_img.shape[1]\n",
    "    padding = window_size//2\n",
    "    count = 0.\n",
    "    N = window_size**2*(height-2*padding)*(width-2*padding)\n",
    "    with Pool(num_process) as pool:\n",
    "        r = pool.map(partial(add_row_calculation, N=N, height=height, width=width, padding=padding, window_size=window_size),\n",
    "                     range(padding, height-padding))\n",
    "    return np.sum(r)\n",
    "\n",
    "def dummy_computation_vectorized(left_img, right_img, window_size):\n",
    "    left_view = sliding_window_view(left_img, (window_size, window_size))\n",
    "    right_view = sliding_window_view(right_img, (window_size, window_size))\n",
    "    return np.average(left_view - right_view)\n",
    "    \n",
    "left_img = cv2.imread(\"Data/scene1.row3.col3.ppm\", cv2.IMREAD_GRAYSCALE).astype(np.float64)\n",
    "right_img = cv2.imread(\"Data/scene1.row3.col4.ppm\", cv2.IMREAD_GRAYSCALE).astype(np.float64)\n",
    "window_size = 3\n",
    "num_process = 12\n",
    "t1 = time()\n",
    "print(\"Naive method result: \\t\\t\", dummy_computation(left_img, right_img, window_size))\n",
    "t2 = time()\n",
    "# parallel_computation = DummyComputationParallized(left_img, right_img, window_size)\n",
    "# print(\"Parallelized method result: \\t\\t\", parallel_computation.run())\n",
    "print(\"Parallelized method result: \\t\", dummy_computation_parallelized(left_img, right_img, window_size, num_process))\n",
    "t3 = time()\n",
    "print(\"Vectorized method result: \\t\", dummy_computation_vectorized(left_img, right_img, window_size))\n",
    "t4 = time()\n",
    "print(f\"Naive method time: \\t\\t{t2-t1} seconds\")\n",
    "print(f\"Parallelized method time: \\t{t3-t2} seconds\")\n",
    "print(f\"Vectorized method time: \\t{t4-t3} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Disparity between a pair of rectified images\n",
    "\n",
    "<span style='color:Green'> - Write a function called 'stereo_computation' that computes the disparity between a pair of rectified images </span>\n",
    "\n",
    "Computational Efficiency is not a requirement but a plus.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/8356912/151308788-854ad073-48b3-495a-af4c-dea99f4343d9.png)\n",
    "\n",
    "**Optional:** You can also compute the depth map knowing *a priori* the focal length and baseline of the camera configuration (source: https://vision.middlebury.edu/stereo/data/scenes2014/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stereo_computation(left_img, right_img, min_disparity, max_disparity, win_size, method, **method_kwargs):\n",
    "    \"\"\"\n",
    "    :param left_img: \n",
    "    :param right_img: \n",
    "    :param min_disparity: \n",
    "    :param max_disparity: \n",
    "    :param win_size: \n",
    "    :param method: \n",
    "    :param method_kwargs\n",
    "    :return: \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Sum of Squared Differences Cost\n",
    "__Data images: 'scene1.row3.col3.ppm','scene1.row3.col4.ppm'  \n",
    "Disparity ground truth: 'truedisp.row3.col3.pgm'__\n",
    "\n",
    "In this part we ask to implement only the SSD cost\n",
    "\n",
    "\n",
    "<span style='color:Green'> - Evaluate the results changing the window size (e.g. 3x3, 9x9, 21x21,31x31) and the Mean Square Error (MSE). Comment the results.  </span>\n",
    "\n",
    " **Note 1**: Use grayscale images  \n",
    " \n",
    " **Note 2**: For this first set of images use 0 as minimum disparity and 16 as the the maximum one.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/8356912/151308710-679e9b2c-8dfc-48da-8378-a844b10ef4f7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Normalized  Cross Correlation Cost\n",
    "\n",
    "<span style='color:Green'> - Complete the previous function by adding the implementation of the NCC cost. </span>\n",
    "\n",
    "<span style='color:Green'>  Evaluate the results changing the window size (e.g. 3x3, 9x9, 21x21,31x31). Comment the results. </span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Apply to facade images\n",
    "\n",
    "Data images: '0001_rectified_s.png','0002_rectified_s.png'\n",
    "\n",
    "<span style='color:Green'> - Test the functions implemented in the previous section with the facade images. Try different matching costs and window sizes and comment the results. </span>\n",
    "    \n",
    "Notice that in this new data the minimum and maximum disparities may change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Adaptive support weights\n",
    "\n",
    "\n",
    "<span style='color:Green'> - Modify the 'stereo_computation' so that you can use adaptive support weights considering both **proximity** and **similarity**</span>\n",
    "\n",
    "- Use the original [Exponential](https://ieeexplore.ieee.org/document/1597121#:~:text=Abstract%3A%20We%20present%20a%20new,to%20reduce%20the%20image%20ambiguity.) filters and [Bilateral](https://www.ipol.im/pub/art/2015/123/) Filtering. How are they different?\n",
    "- How are the support weights adaptive? Plot the support weights for exemplary cases that illustrate the adaptive nature.\n",
    "- Implement the support weights for color and black and white images. What similarity metric for black and white can you use? \n",
    "- Make a small comparison of the effect of the support weights parameters, and compare them to the previous results.\n",
    "   \n",
    "Reference paper: Yoon and Kweon, \"Adaptive Support-Weight Approach for Correspondence Search\", IEEE PAMI 2006\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/8356912/151323058-04124700-7c77-46bc-8b25-6ac1473300ce.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Optional Tasks**\n",
    "\n",
    "\n",
    "## Optional 1:  Depth computation with Plane Sweeping\n",
    "\n",
    "<span style='color:Green'>  Implement the plane sweeping method explained in class. You can implemented naively or with the computational efficiency details explained on the paper</span>\n",
    " \n",
    " \"Given a plane in space, it is possible to project both images onto\n",
    "it using projective texture mapping. If the plane is located\n",
    "at the same depth as the recorded scene, pixels from both\n",
    "images should be consistent.\" [Multi-Resolution Real-Time Stereo on Commodity Graphics Hardware](https://people.inf.ethz.ch/pomarc/pubs/YangCVPR03.pdf)\n",
    "\n",
    "- Be clear about what information from the cameras and scene is required\n",
    "- Implement the algorithm for two Cameras. Test it on the Middlebury images. Compare quantitatively this method and the previously developed ones for depth estimates with respect to the ground truth depth.\n",
    "- What changes you will have to make to process unrectified images? No need to implement them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional 2:  New view synthesis using view morphing\n",
    "\n",
    "In this task you are asked to implement part of the new view synthesis method presented in the following paper: S. Seitz, and C. Dyer, View morphing, Proc. ACM SIGGRAPH 1996.\n",
    "\n",
    "You will use a pair of rectified stereo images (no need for prewarping and postwarping stages) and their corresponding ground truth disparities (folder \"new_view\"). Remember to take into account occlusions as explained in the lab session. Once done you can apply the code to the another pair of rectified images provided in the material and use the estimated disparities with previous methods.\n",
    "\n",
    "![new_views_morph](https://user-images.githubusercontent.com/8356912/151237907-8944e3d2-f84d-4707-a63c-357f005b4823.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional 3:  Depth map fusion review\n",
    "\n",
    "This task is intended to bridge the concepts from novel and traditional applications. You must not implement any code but you are asked to review 2 papers:\n",
    "\n",
    "- [B. Curless and M. Levoy. A Volumetric Method for Building Complex Models from Range Images. In Proc. SIGGRAPH, 1996](https://graphics.stanford.edu/papers/volrange/volrange.pdf).\n",
    "- [RoutedFusion: Learning Real-time Depth Map Fusion Silvan Weder, Johannes L. Sch√∂nberger, Marc Pollefeys, Martin R. Oswald May 2020](https://www.silvanweder.com/publications/routed-fusion/)\n",
    "\n",
    "You should provide a brief report Max 3 pages, less is preffered, where you make a detailed comparison between the two methods.\n",
    "- Pay special attention to what concepts are shared between both approaches along the entire processing pipeline.\n",
    "- What role play NN based function approximators in the entire processing pipeline of the second paper.How do they relate to the first approach? \n",
    "\n",
    "![image](https://user-images.githubusercontent.com/8356912/151363747-ae8ab96a-8eb9-4b56-b797-2eddf72f3e9c.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
